{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"Change to the data folder\"\"\"\n",
    "new_path = \"./new_train/new_train\"\n",
    "#new_path = r\"C:\\Users\\mnisyu\\Desktop\\CSE151B competition\\cse151b-spring\\\\\"\n",
    "#new_path = \"..\"\n",
    "\n",
    "# number of sequences in each dataset\n",
    "# train:205942  val:3200 test: 36272 \n",
    "# sequences sampled at 10HZ rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, data_path: str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n",
    "        self.pkl_list.sort()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pkl_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pkl_path = self.pkl_list[idx]\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "# intialize a dataset\n",
    "val_dataset  = ArgoverseDataset(data_path=new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205942"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ArgoverseDataset(data_path=\"./new_val_in/new_val_in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loader to enable batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 4\n",
    "\n",
    "def my_collate(batch):\n",
    "    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n",
    "    inp = [numpy.dstack([scene['p_in'], scene['v_in']]) for scene in batch]\n",
    "    out = [numpy.dstack([scene['p_out'], scene['v_out']]) for scene in batch]\n",
    "    inp = torch.LongTensor(inp)\n",
    "    out = torch.LongTensor(out)\n",
    "    return [inp, out]\n",
    "\n",
    "val_loader = DataLoader(val_dataset,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a, b = next(iter(val_loader))\n",
    "len(a) # 4 batches/scenes\n",
    "len(b) # 4 batches/scenees\n",
    "len(a[0]) # 60 vehicles\n",
    "len(b[0]) # 60 vehicles\n",
    "len(a[0][0]) # 19 timestamps\n",
    "len(b[0][0]) # 30 timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_dim, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_dim, num_layers=self.num_layers)\n",
    "        self.hidden = None\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Push through RNN layer (the ouput is irrelevant)\n",
    "        _, self.hidden = self.lstm(inputs, self.hidden)\n",
    "        return self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        # input_size=1 since the output are single values\n",
    "        self.lstm = nn.LSTM(1, hidden_dim, num_layers=num_layers)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, outputs, hidden, criterion):\n",
    "        batch_size, num_steps = outputs.shape\n",
    "        # Create initial start value/token\n",
    "        input = torch.tensor([[0.0]] * batch_size, dtype=torch.float)\n",
    "        # Convert (batch_size, output_size) to (seq_len, batch_size, output_size)\n",
    "        input = input.unsqueeze(0)\n",
    "\n",
    "        loss = 0\n",
    "        for i in range(num_steps):\n",
    "            # Push current input through LSTM: (seq_len=1, batch_size, input_size=1)\n",
    "            output, hidden = self.lstm(input, hidden)\n",
    "            # Push the output of last step through linear layer; returns (batch_size, 1)\n",
    "            output = self.out(output[-1])\n",
    "            # Generate input for next step by adding seq_len dimension (see above)\n",
    "            input = output.unsqueeze(0)\n",
    "            # Compute loss between predicted value and true value\n",
    "            loss += criterion(output, outputs[:, i])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.44670340418815613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnisyu\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # 5 is the number of features of your data points\n",
    "    encoder = Encoder(5, 128)\n",
    "    decoder = Decoder(128)\n",
    "    # Create optimizers for encoder and decoder\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Some toy data: 2 sequences of length 10 with 5 features for each data point\n",
    "    inputs = [\n",
    "        [\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "        ],\n",
    "        [\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "            [0.5, 0.2, 0.3, 0.4, 0.1],\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    inputs = torch.tensor(np.array(inputs), dtype=torch.float)\n",
    "    # Convert (batch_size, seq_len, input_size) to (seq_len, batch_size, input_size)\n",
    "    inputs = inputs.transpose(1,0)\n",
    "\n",
    "    # 2 sequences (to match the batch size) of length 6 (for the 6h into the future)\n",
    "    outputs = [ [0.1, 0.2, 0.3, 0.1, 0.2, 0.3], [0.3, 0.2, 0.1, 0.3, 0.2, 0.1] ]\n",
    "    outputs = torch.tensor(np.array(outputs), dtype=torch.float)\n",
    "\n",
    "    #\n",
    "    # Do one complete forward & backward pass\n",
    "    #\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    # Reset hidden state of encoder for current batch\n",
    "    encoder.hidden = encoder.init_hidden(inputs.shape[1])\n",
    "    # Do forward pass through encoder\n",
    "    hidden = encoder(inputs)\n",
    "    # Do forward pass through decoder (decoder gets hidden state from encoder)\n",
    "    loss = decoder(outputs, hidden, criterion)\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    # Update parameters\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 19, 4])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19, 4])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3277, 1947,    0,    0])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4\n",
    "hidden_dim = 10\n",
    "n_layers = 1\n",
    "lstm_layer = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 1\n",
    "\n",
    "inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "hidden = (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2699,  1.0593, -0.3922, -1.3150, -0.8106,  0.8438,  1.9791,\n",
       "           -0.3544,  1.3327, -0.8712]]]),\n",
       " tensor([[[-0.4122, -0.9752, -0.8629, -0.8413,  1.5831, -1.3641,  1.2202,\n",
       "           -1.5006,  1.1591,  1.6278]]]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   5,   5],\n",
       "       [ 10,  15,  25],\n",
       "       [ 20,  25,  45],\n",
       "       [ 30,  35,  65],\n",
       "       [ 40,  45,  85],\n",
       "       [ 50,  55, 105],\n",
       "       [ 60,  65, 125],\n",
       "       [ 70,  75, 145],\n",
       "       [ 80,  85, 165],\n",
       "       [ 90,  95, 185]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the batch of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-e34a6e23e746>:7: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  inp = torch.LongTensor(inp)\n",
      "<ipython-input-13-e34a6e23e746>:8: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  out = torch.LongTensor(out)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "agent_id = 0\n",
    "\n",
    "def show_sample_batch(sample_batch, agent_id):\n",
    "    \"\"\"visualize the trajectory for a batch of samples with a randon agent\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i, agent_id,:,0], inp[i, agent_id,:,1])\n",
    "        axs[i].scatter(out[i, agent_id,:,0], out[i, agent_id,:,1])\n",
    "\n",
    "        \n",
    "for i_batch, sample_batch in enumerate(val_loader):\n",
    "    inp, out = sample_batch\n",
    "    \"\"\"TODO:\n",
    "      Deep learning model\n",
    "      training routine\n",
    "    \"\"\"\n",
    "    show_sample_batch(sample_batch, agent_id)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
