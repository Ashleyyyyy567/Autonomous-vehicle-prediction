{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport os, os.path \nimport numpy \nimport pickle\nfrom glob import glob\n\n\"\"\"Change to the data folder\"\"\"\nnew_path = \"./new_train/new_train\"\n#new_path = r\"C:\\Users\\mnisyu\\Desktop\\CSE151B competition\\cse151b-spring\\\\\"\n#new_path = \"..\"\n\n# number of sequences in each dataset\n# train:205942  val:3200 test: 36272 \n# sequences sampled at 10HZ rate","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Create a dataset class ","metadata":{}},{"cell_type":"code","source":"import pickle\n\nclass ArgoverseDataset(Dataset):\n    \"\"\"Dataset class for Argoverse\"\"\"\n    def __init__(self, data_path: str, transform=None):\n        super(ArgoverseDataset, self).__init__()\n        self.data_path = data_path\n        self.transform = transform\n\n        self.pkl_list = glob(os.path.join(self.data_path, '*'))\n        self.pkl_list.sort()\n        \n    def __len__(self):\n        return len(self.pkl_list)\n\n    def __getitem__(self, idx):\n\n        pkl_path = self.pkl_list[idx]\n        with open(pkl_path, 'rb') as f:\n            data = pickle.load(f)\n            \n        if self.transform:\n            data = self.transform(data)\n\n        return data\n\n\n# intialize a dataset\ntrain_data  = ArgoverseDataset(data_path=\"../input/kaggle-competitions-download-c-cse151bspring/new_train/new_train/\")\ntest_data  = ArgoverseDataset(data_path=\"../input/kaggle-competitions-download-c-cse151bspring/new_val_in/new_val_in/\")","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Create a loader to enable batch processing","metadata":{}},{"cell_type":"markdown","source":"# By Lehan Li","metadata":{}},{"cell_type":"code","source":"import itertools\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nbatch_sz = 4\n\ndef my_collate(batch):\n    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n    inp = [np.dstack([scene['p_in'][:,:,0].reshape(60*19),scene['p_in'][:,:,1].reshape(60*19),scene['v_in'][:,:,0].reshape(60*19),scene['v_in'][:,:,1].reshape(60*19)]).reshape(60*19,4) for scene in batch]\n    \n    indexs = [np.where(scene['agent_id'] == scene['track_id'][:,0].reshape(60))[0][0] for scene in batch]\n    #p_in= [scene['p_in'][index] for (scene,index) in zip(batch, indexs)]\n    #v_in = [scene['v_in'][index] for (scene,index) in zip(batch, indexs)]\n    #inp = [np.concatenate((p[:,0],p[:,1],v[:,0],v[:,1])).reshape(76,1) for(p,v) in zip(p_in,v_in)]\n    \n    p_out = [scene['p_out'][index] for (scene,index) in zip(batch, indexs)]\n    v_out = [scene['v_out'][index] for (scene,index) in zip(batch, indexs)]\n    out = [np.concatenate((p[:,0],p[:,1],v[:,0],v[:,1])) for(p,v) in zip(p_out,v_out)]\n    \n    inp = torch.FloatTensor(inp)\n    out = torch.FloatTensor(out)\n    return [inp, out]\n\ntrain_loader = DataLoader(train_data,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0, drop_last=True)","metadata":{"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"_, (example_datas, labels) = next(enumerate(train_loader))\nexample_datas.size()\n#labels.size()","metadata":{"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 1140, 4])"},"metadata":{}}]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.lstm = torch.nn.LSTM(4, 1, 4, dropout=0.2)\n        self.conv1 = torch.nn.Conv1d(1140, 521, 1)\n        self.conv2 = torch.nn.Conv1d(521, 256, 1)\n        self.conv3 = torch.nn.Conv1d(256, 120, 1)\n\n        self.bn1 = torch.nn.BatchNorm1d(521)\n        self.bn2 = torch.nn.BatchNorm1d(256)\n\n        self.drop = torch.nn.Dropout(0.2)\n    def forward(self, inputs):\n        x = inputs\n        x = self.lstm(x)[0]\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.drop(x)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.drop(x)\n        x = self.conv3(x)\n        return x.reshape(4,120)","metadata":{"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"net = Net()\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\nfor epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 200 == 199:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 200))\n            running_loss = 0.0\n","metadata":{"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"[1,   200] loss: 1252260.297\n[1,   400] loss: 1607374.644\n[1,   600] loss: 1523639.462\n[1,   800] loss: 2148098.919\n[1,  1000] loss: 2100286.278\n[1,  1200] loss: 1910400.954\n[1,  1400] loss: 1861766.410\n[1,  1600] loss: 1815994.726\n[1,  1800] loss: 1673669.487\n[1,  2000] loss: 1617192.509\n[1,  2200] loss: 1545930.605\n[1,  2400] loss: 1540218.615\n[1,  2600] loss: 1441493.768\n[1,  2800] loss: 1400581.814\n[1,  3000] loss: 1334502.979\n[1,  3200] loss: 1289691.495\n[1,  3400] loss: 1209620.647\n[1,  3600] loss: 1250113.216\n[1,  3800] loss: 1204860.927\n[1,  4000] loss: 1149831.371\n[1,  4200] loss: 1066621.613\n[1,  4400] loss: 1081109.162\n[1,  4600] loss: 1073472.740\n[1,  4800] loss: 969267.841\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-63-59f2533d3e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-52-788cfc8bf8ff>\u001b[0m in \u001b[0;36mmy_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"class LSTM(nn.Module):\n    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n        super().__init__()\n        self.hidden_layer_size = hidden_layer_size\n\n        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n\n        self.linear = nn.Linear(hidden_layer_size, output_size)\n\n        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n                            torch.zeros(1,1,self.hidden_layer_size))\n\n    def forward(self, input_seq):\n        lstm_out, self.hidden_cell = self.lstm(input_seq, self.hidden_cell)\n        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n        return predictions[-1]","metadata":{"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"LSTM = LSTM()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\nfor epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n\nprint('Finished Training')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport numpy as np","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import itertools\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nbatch_sz = 100\n\ndef my_collate(batch):\n    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n    #inp = [np.dstack([scene['p_in'][:,:,0].reshape(60*19),scene['p_in'][:,:,1].reshape(60*19),scene['v_in'][:,:,0].reshape(60*19),scene['v_in'][:,:,1].reshape(60*19)]).reshape(60*19,4) for scene in batch]\n    \n    \n    x_in = [np.concatenate([scene['p_in'][:,:,0].reshape(-1),scene['v_in'][:,:,0].reshape(-1), scene['lane'][:,0], scene['lane_norm'][:,0]]) for scene in batch]\n    y_in = [np.concatenate([scene['p_in'][:,:,1].reshape(-1),scene['v_in'][:,:,1].reshape(-1), scene['lane'][:,1], scene['lane_norm'][:,1]]) for scene in batch]\n    max_length = np.max([len(x) for x in x_in])\n    diff_length = [max_length - len(x) for x in x_in]\n    \n    xs = [np.concatenate([x, np.zeros(diff)]).reshape(-1) for (x,diff) in zip(x_in, diff_length)]\n    ys = [np.concatenate([y, np.zeros(diff)]).reshape(-1) for (y,diff) in zip(y_in, diff_length)]\n    inp = [np.dstack([x,y]).reshape(-1,2) for (x, y) in zip(xs,ys)]\n            \n    indexs = [np.where(scene['agent_id'] == scene['track_id'][:,0].reshape(60))[0][0] for scene in batch]\n    p_out = [scene['p_out'][index] for (scene,index) in zip(batch, indexs)]\n    v_out = [scene['v_out'][index] for (scene,index) in zip(batch, indexs)]\n    out = [np.concatenate((p[:,0],p[:,1],v[:,0],v[:,1])) for(p,v) in zip(p_out,v_out)]\n    \n    inp = torch.FloatTensor(inp)\n    out = torch.FloatTensor(out)\n    return [inp, out]\n\ntrain_loader = DataLoader(train_data,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0, drop_last=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nbatch_sz = 100\n\ndef my_collate(batch):\n    \"\"\" collate lists of samples into batches, create [ batch_sz x agent_sz x seq_len x feature] \"\"\"\n    #inp = [np.dstack([scene['p_in'][:,:,0].reshape(60*19),scene['p_in'][:,:,1].reshape(60*19),scene['v_in'][:,:,0].reshape(60*19),scene['v_in'][:,:,1].reshape(60*19)]).reshape(60*19,4) for scene in batch]\n    \n    agents = [np.where(scene['car_mask'])[0] for scene in batch]\n    \n    p_in = [scene['p_in'][agent].reshape(-1,2) for (scene, agent) in zip(batch,agents)]\n    v_in = [scene['v_in'][agent].reshape(-1,2) for (scene, agent) in zip(batch,agents)]\n    x_in = [np.concatenate([p[:,0].reshape(-1),v[:,0].reshape(-1), scene['lane'][:,0], scene['lane_norm'][:,0]]) for (scene,p,v) in zip(batch, p_in, v_in)]\n    y_in = [np.concatenate([p[:,1].reshape(-1),v[:,1].reshape(-1), scene['lane'][:,1], scene['lane_norm'][:,1]]) for (scene,p,v) in zip(batch, p_in, v_in)]\n    \n    max_length = np.max([len(x) for x in x_in])\n    diff_length = [max_length - len(x) for x in x_in]\n    \n    xs = [np.concatenate([x, np.zeros(diff)]).reshape(-1) for (x,diff) in zip(x_in, diff_length)]\n    ys = [np.concatenate([y, np.zeros(diff)]).reshape(-1) for (y,diff) in zip(y_in, diff_length)]\n    inp = [np.dstack([x,y]).reshape(-1,2) for (x, y) in zip(xs,ys)]\n    \n    indexs = [np.where(scene['agent_id'] == scene['track_id'][:,0].reshape(60))[0][0] for scene in batch]\n    p_out = [scene['p_out'][index] for (scene,index) in zip(batch, indexs)]\n    v_out = [scene['v_out'][index] for (scene,index) in zip(batch, indexs)]\n    out = [np.concatenate((p[:,0],p[:,1],v[:,0],v[:,1])) for(p,v) in zip(p_out,v_out)]\n    \n    inp = torch.FloatTensor(inp)\n    out = torch.FloatTensor(out)\n    return [inp, out]\n\ntrain_loader = DataLoader(train_data,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0, drop_last=True)","metadata":{"trusted":true},"execution_count":215,"outputs":[]},{"cell_type":"code","source":"_, (example_datas, labels) = next(enumerate(train_loader))\nexample_datas.size()","metadata":{"trusted":true},"execution_count":210,"outputs":[{"execution_count":210,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 1288, 2])"},"metadata":{}}]},{"cell_type":"code","source":"_, (x_in, y_in, max_length, diff_length) = next(enumerate(train_loader))\n[np.concatenate([x, np.zeros(diff)]) for (x,diff) in zip(x_in, diff_length)]\ntype(x_in[0])\n#[np.concatenate(x, np.zeros(3)) for (x) in x_in]\nnp.concatenate([x_in[0], np.zeros(3)])","metadata":{"trusted":true},"execution_count":183,"outputs":[{"execution_count":183,"output_type":"execute_result","data":{"text/plain":"array([3277.29638672, 3277.29614258, 3277.29614258, ...,    0.        ,\n          0.        ,    0.        ])"},"metadata":{}}]},{"cell_type":"code","source":"agents = np.where(train_data[0]['car_mask'])[0]\np_in = train_data[0]['p_in'][agents]\nv_in = train_data[0]['v_in'][agents]\nv_in.shape\nnp.dstack([v_in[:,:,0].reshape(-1), v_in[:,:,0].reshape(-1)])\n\nagents = [np.where(train_data[scene]['car_mask'])[0] for scene in [0,1,2]]\np_in = [train_data[scene]['p_in'][agent].reshape(-1,2) for (scene, agent) in zip([0,1,2], agents)]\np_in[0][:,0]\n\na = train_data[1]['lane'].shape[0]\n\nb = np.zeros(270)[:a] = train_data[1]['lane'][:,0]\nb","metadata":{"trusted":true},"execution_count":126,"outputs":[{"execution_count":126,"output_type":"execute_result","data":{"text/plain":"array([825.8388 , 825.92194, 826.0051 , 826.08826, 826.17145, 826.2546 ,\n       826.33777, 826.4209 , 826.504  , 831.40796, 831.2262 , 831.04443,\n       830.8627 , 830.6809 , 830.4992 , 830.31744, 830.1357 , 829.9539 ,\n       828.6891 , 828.9834 , 829.5268 , 830.3264 , 831.2993 , 832.4175 ,\n       833.6703 , 835.01324, 836.374  , 820.5563 , 821.69073, 822.74084,\n       823.67126, 824.42535, 824.9891 , 825.3893 , 825.62854, 825.7734 ,\n       828.46796, 828.1686 , 827.6147 , 826.8077 , 825.8071 , 824.6502 ,\n       823.0497 , 820.97314, 818.57385, 821.4527 , 823.1304 , 824.5425 ,\n       825.69965, 826.6544 , 827.5103 , 827.99567, 828.1028 , 827.93744,\n       830.8283 , 829.12164, 827.8425 , 826.9521 , 826.2958 , 825.84845,\n       825.6743 , 825.69086, 825.7734 , 809.2443 , 809.9523 , 810.66016,\n       811.36804, 812.076  , 812.7839 , 813.4918 , 814.1997 , 814.90765,\n       825.01385, 825.2128 , 825.7773 , 826.8167 , 828.13336, 829.77704,\n       831.69525, 833.8893 , 836.2484 , 825.787  , 825.8006 , 825.8143 ,\n       825.8279 , 825.8415 , 825.85516, 825.8688 , 825.8824 , 825.896  ,\n       832.3973 , 833.6479 , 834.98645, 836.4628 , 837.9442 , 839.4256 ,\n       840.89166, 842.3524 , 843.8132 , 839.1532 , 837.37463, 835.5725 ,\n       833.75507, 831.93756, 830.2149 , 828.57025, 827.23553, 826.504  ,\n       837.16125, 838.07404, 838.9869 , 839.8997 , 840.81256, 841.7254 ,\n       842.63824, 843.551  , 844.46387, 828.51306, 828.4411 , 828.36914,\n       828.2972 , 828.2252 , 828.15326, 828.0813 , 828.0094 , 827.93744,\n       824.9905 , 825.0883 , 825.1862 , 825.28406, 825.3819 , 825.4798 ,\n       825.57764, 825.6755 , 825.7734 , 829.4245 , 829.3243 , 829.22406,\n       829.1239 , 829.0237 , 828.9235 , 828.8233 , 828.72314, 828.6229 ,\n       825.9682 , 826.0404 , 826.11255, 826.18475, 826.25696, 826.3291 ,\n       826.4013 , 826.4735 , 826.54565, 850.07666, 849.09467, 848.0459 ,\n       846.9843 , 845.8849 , 844.69965, 843.4757 , 842.192  , 840.9083 ,\n       829.90625, 829.8585 , 829.81085, 829.7631 , 829.71545, 829.6677 ,\n       829.62006, 829.5724 , 829.52466, 826.4517 , 826.3578 , 826.2638 ,\n       826.16986, 826.0759 , 825.98193, 825.888  , 825.79407, 825.7557 ,\n       839.71326, 838.5181 , 837.19885, 835.71027, 833.99615, 832.3141 ,\n       831.06683, 830.34576, 829.9539 , 826.14215, 827.00146, 828.7976 ,\n       831.6047 , 834.65295, 837.494  , 839.8364 , 841.83636, 843.8132 ,\n       821.2805 , 823.1672 , 825.0539 , 826.94055, 828.8273 , 830.7139 ,\n       832.60065, 834.4873 , 836.374  , 821.33765, 823.20154, 825.06537,\n       826.9292 , 828.793  , 830.65686, 832.5207 , 834.3846 , 836.2484 ,\n       837.2886 , 838.2031 , 839.1177 , 840.0322 , 840.9468 , 841.8613 ,\n       842.77594, 843.6905 , 844.60504, 809.1078 , 809.8215 , 810.5351 ,\n       811.2487 , 811.9624 , 812.676  , 813.3897 , 814.10333, 814.81696,\n       815.3344 , 815.8518 , 816.36926, 816.8867 , 817.4041 , 817.9216 ,\n       818.43896, 818.9564 , 819.4738 , 815.40607, 815.90454, 816.403  ,\n       816.9015 , 817.39996, 817.89844, 818.3969 , 818.8954 , 819.3938 ,\n       828.6187 , 828.6145 , 828.6103 , 828.6061 , 828.60187, 828.59766,\n       828.59344, 828.58923, 828.585  ], dtype=float32)"},"metadata":{}}]},{"cell_type":"code","source":"max_lane = 0\nmax_lane_norm = 0\nfor scene in train_data: \n    if scene['lane'].shape[0] > max_lane: \n        max_line = scene['lane'].shape[0]\n    if scene['lane_norm'].shape[0] > max_lane_norm:\n        max_lane_norm = scene['lane_norm'].shape[0]\n","metadata":{"trusted":true},"execution_count":116,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-116-49806e3115bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_lane\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_lane_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mscene\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lane'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_lane\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmax_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lane'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-3e387617fcc1>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mpkl_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpkl_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"class Encoder(nn.Module):\n\n    def __init__(self, input_size, hidden_dim, num_layers=1):\n        super(Encoder, self).__init__()\n\n        self.input_size = input_size\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(self.input_size, self.hidden_dim, num_layers=self.num_layers)\n        self.hidden = None\n\n    def init_hidden(self, batch_size):\n        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n                torch.zeros(self.num_layers, batch_size, self.hidden_dim))\n\n    def forward(self, inputs):\n        # Push through RNN layer (the ouput is irrelevant)\n        _, self.hidden = self.lstm(inputs, self.hidden)\n        return self.hidden","metadata":{"trusted":true},"execution_count":212,"outputs":[]},{"cell_type":"code","source":"class Decoder(nn.Module):\n\n    def __init__(self, hidden_dim, num_layers=1):\n        super(Decoder, self).__init__()\n        # input_size=1 since the output are single values\n        self.lstm = nn.LSTM(1, hidden_dim, num_layers=num_layers)\n        self.out = nn.Linear(hidden_dim, 1)\n\n    def forward(self, outputs, hidden, criterion):\n        batch_size, num_steps = outputs.shape\n        # Create initial start value/token\n        input = torch.tensor([[0.0]] * batch_size, dtype=torch.float)\n        # Convert (batch_size, output_size) to (seq_len, batch_size, output_size)\n        input = input.unsqueeze(0)\n\n        loss = 0\n        for i in range(num_steps):\n            # Push current input through LSTM: (seq_len=1, batch_size, input_size=1)\n            #print(hidden[0].size())\n            output, hidden = self.lstm(input, hidden)\n            #print('here')\n            # Push the output of last step through linear layer; returns (batch_size, 1)\n            output = self.out(output[-1])\n            # Generate input for next step by adding seq_len dimension (see above)\n            input = output.unsqueeze(0)\n            # Compute loss between predicted value and true value\n            loss += criterion(output, outputs[:, i])\n        return loss","metadata":{"trusted":true},"execution_count":213,"outputs":[]},{"cell_type":"code","source":"encoder = Encoder(2, 120)\ndecoder = Decoder(120)\ncriterion = nn.MSELoss()\nencoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\ndecoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\na = 0\nfor epoch in range(5):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        if (len(data[0]) < 4): \n            break\n        inputs,labels = data\n        inputs = inputs.transpose(1,0)\n        \n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n        \n        #print(inputs.shape[1])\n        encoder.hidden = encoder.init_hidden(inputs.shape[1])\n        #print(encoder.hidden[0].size())\n        # Do forward pass through encoder\n        hidden = encoder(inputs)\n        #print(hidden[0].size())\n        #a = hidden\n        # Do forward pass through decoder (decoder gets hidden state from encoder)\n        loss = decoder(labels, hidden, criterion)\n        # Backpropagation\n        loss.backward()\n        # Update parameters\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n        \n        running_loss += loss.item()\n        if i % 200 == 199:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 200))\n            running_loss = 0.0","metadata":{"trusted":true},"execution_count":216,"outputs":[{"name":"stdout","text":"285770176.0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-216-3bb79af9dca5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# End of Lehan Li","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":99,"outputs":[{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 4, 120])"},"metadata":{}}]},{"cell_type":"code","source":"if __name__ == '__main__':\n\n    # 5 is the number of features of your data points\n\n    # Create optimizers for encoder and decoder\n    encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n    criterion = nn.MSELoss()\n\n    # Some toy data: 2 sequences of length 10 with 5 features for each data point\n    inputs = [\n        [\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n        ],\n        [\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n            [0.5, 0.2, 0.3, 0.4, 0.1],\n        ]\n    ]\n\n    inputs = torch.tensor(np.array(inputs), dtype=torch.float)\n    # Convert (batch_size, seq_len, input_size) to (seq_len, batch_size, input_size)\n    inputs = inputs.transpose(1,0)\n    print(inputs.size())\n\n    # 2 sequences (to match the batch size) of length 6 (for the 6h into the future)\n    outputs = [ [0.1, 0.2, 0.3, 0.1, 0.2, 0.3], [0.3, 0.2, 0.1, 0.3, 0.2, 0.1] ]\n    outputs = torch.tensor(np.array(outputs), dtype=torch.float)\n\n    print(outputs.size())\n    #\n    # Do one complete forward & backward pass\n    #\n    # Zero gradients of both optimizers\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n    # Reset hidden state of encoder for current batch\n    encoder.hidden = encoder.init_hidden(inputs.shape[1])\n    # Do forward pass through encoder\n    hidden = encoder(inputs)\n    # Do forward pass through decoder (decoder gets hidden state from encoder)\n    loss = decoder(outputs, hidden, criterion)\n    # Backpropagation\n    loss.backward()\n    # Update parameters\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n    print(\"Loss:\", loss.item())","metadata":{"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"torch.Size([10, 2, 5])\ntorch.Size([2, 6])\nLoss: 0.2605845332145691\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize the batch of sequences","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport random\n\nagent_id = 0\n\ndef show_sample_batch(sample_batch, agent_id):\n    \"\"\"visualize the trajectory for a batch of samples with a randon agent\"\"\"\n    inp, out = sample_batch\n    batch_sz = inp.size(0)\n    agent_sz = inp.size(1)\n    \n    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n    fig.subplots_adjust(hspace = .5, wspace=.001)\n    axs = axs.ravel()   \n    for i in range(batch_sz):\n        axs[i].xaxis.set_ticks([])\n        axs[i].yaxis.set_ticks([])\n        \n        # first two feature dimensions are (x,y) positions\n        axs[i].scatter(inp[i, agent_id,:,0], inp[i, agent_id,:,1])\n        axs[i].scatter(out[i, agent_id,:,0], out[i, agent_id,:,1])\n\n        \nfor i_batch, sample_batch in enumerate(val_loader):\n    inp, out = sample_batch\n    \"\"\"TODO:\n      Deep learning model\n      training routine\n    \"\"\"\n    show_sample_batch(sample_batch, agent_id)\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}